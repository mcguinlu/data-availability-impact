---
title: ""
bibliography: 
  - references.bib
  - references2.bib
  - references_packages.bib
csl: bmj.csl
output:
  bookdown::word_document2:
      toc: false
      toc_depth: 3
      reference_docx: word-styles-reference-01.docx
---

``` {r, echo = FALSE, warning=FALSE, message=FALSE }
library(dplyr)
library(kableExtra)
library(rio)
library(MASS)
library(here)
library(epitools)
library(flextable)
library(ftExtra)

source(here("R","0_library.R"))
source(here("R","3_results.R"))

knitr::opts_knit$set(eval.after = "tab.cap")
knitr::opts_chunk$set(echo=FALSE, warning = FALSE, message = FALSE)

```

# Front Matter
__Title: __  
All I want for Christmas is you(r data): the impact of journal data-sharing policies on data availability statements

&nbsp;

__Authors and Affiliations__

Luke A McGuinness^1,2^ (ORCID: 0000-0001-8730-9761), Athena L Sheppard^3^ (ORCID: 0000-0003-1564-0740)

(1) Population Health Sciences, Bristol Medical School, University of Bristol, Bristol, UK
(2) MRC Integrative Epidemiology Unit at the University of Bristol, Bristol, UK
(3)	Department of Health Sciences, University of Leicester, Leicester, UK

&nbsp;

__Corresponding author:__  

Luke McGuinness; Bristol Medical School, University of Bristol,
Canynge Hall, 39 Whatley Road, Bristol, BS8 2PS, United Kingdom; luke.mcguinness@bristol.ac.uk

&nbsp;

__Keywords__

Reproducibility; Data sharing; Data availability statements; Journalology; Preprints; Descriptive study

#####

# Abstract

__Objective__   
To determine whether medRxiv data availability statements are "open" or "closed", and whether this changes on publication based on journal data sharing policy.

__Design__  
Observational study of the data availability statements accompanying preprints posted on the medRxiv repository between 25th June 2019 and 1st May 2020, and their published counterparts.

__Setting__  
medRxiv preprint repository.

__Main outcome measures__  
Distribution of preprinted data availability statements across categories of openness, determined by a prespecified classification system.

Change in "openness" of data availability statements between the preprinted and published versions of the same record, stratified by journal sharing policy.

__Results__    
Of `r n_records` medRxiv preprints included in our sample, of which `r n_available_pre_abstract` were categorized as open, `r n_not_available_abstract` as closed, `r n_excluded_pre ` as not applicable (e.g. editorial, protocol). `r n_pub` preprints were subsequently published, and of these published articles, only `r n_pub_denominator_format` contained a data availability statement. Similar to the preprint stage, most published data availability statements were closed (`r n_available_pub_abstract` open, `r n_not_available_pub_abstract` closed, `r n_excluded_pub` not applicable).

Of the `r n_comparison_abstract` records eligible for the comparison between preprinted and published stages, `r n_policy_open` were published in journals which mandated open data sharing. Data more frequently became available on publication when the journal mandated open data sharing (open at preprint: `r open_at_pre_req`, open at publication: `r open_at_pub_req`) compared to when the journal did not mandate open data sharing (open at preprint: `r open_at_pre_not`, open at publication: `r open_at_pub_not`). 

__Conclusion__    
Requiring that authors submit a data availability statement is a good first step, but is insufficient to ensure data availability. Strict editorial policies that require data sharing (where appropriate) as a condition of publication appear to be effective in making research data available. We would strongly encourage all journal editors to take a moment this holiday season and examine whether their data availability policies are sufficient - although it’s been said many times, many ways; open data helps all.

#####

# Introduction {#intro}

The sharing of data generated by a study is becoming an increasingly important aspect of scientific research.[@packer2018; @taichman2016] Without access to the data, it is harder for other researchers to examine, verify and build on the results of that study.[@krumholz2015] As a result, many journals now require data availability statements. These are dedicated sections of research articles, which are intended to provide readers with important information about whether the data described by the study are available  and if so, where they can be obtained.[@federer2018] 

While requiring data availability statements is an admirable first step for journals to take, a lack of review of the contents of these statements often leads to issues. Many authors claim that their data can be made "available on request", despite previous work establishing that these statements are demonstrably untrue in the majority of case - that when data is requested, it is not actually made available.[@naudet2018; @miyakawa2020; @krawczyk2012] Additionally, previous work found that the availability of data "available on request" declines with article age, indicating that this approach is not a valid long term option for data sharing.[@vines2014] This suggests that requiring data availability statements without a corresponding editorial or peer review of their contents, in line with a strictly enforced data-sharing policy, does not achieve the intended aim of making research data more openly available. However, few journals actually require data sharing as a condition of publication. Of a sample of 318 biomedical journals, only ~20% had a data-sharing policy that required data sharing.[@vasilevsky2017] 

Several previous studies have examined the data availability statements of published articles,[@colavizza2020; @federer2018; @roche2015; @tan2020] but to date, none have examined the statements accompanying preprinted manuscripts, including those hosted on  medRxiv, the preprint repository for manuscripts in the medical, clinical, and related health sciences.[@rawlinson2019a] Given that preprints, particularly those on medRxiv, have impacted the academic discourse around the recent (and ongoing) COVID-19 pandemic to a similar, if not greater, extent than published manuscripts,[@fraser2020] assessing the “openness” of their data availability statements is worthwhile. In addition, by comparing the preprint and published versions of the data availability statements for the same paper, the potential impact of different journal data-sharing policies on data availability can be examined. This study aimed to explore the distribution of data availability statements across a number of categories of "openness" - as listed in Table \@ref(tab:categorylabels) - and to assess the change between preprint and published data availability statements, stratified by journal data-sharing policy. We also intended to examine whether authors planning to make the data available on publication actually do so, and whether data availability statements are sufficient to capture code availability declarations.

#####

``` {r categorylabels, ft.align="center", tab.cap = "Categories used to classify the data availability statements. Examples were taken from preprints included in our sample.[@@ehrlich2019; @septiandri2019; @solis2019; @ebbeling2019; @barry2020; @malpas2019; @knuppel2019; @thompson2019; @moriarty2019a]"}

tab <- rio::import(here("report","table-data","categories.xlsx")) %>%
    replace(is.na(.), "")

# This generates MD citations for the table caption, and copied them to your clipboard, so that you can paste them into the table caption.
tab_md <- tab$Citation %>%
  paste0(collapse = "; ") %>%
  paste0("[@",.,"]")
# writeClipboard(tab_md)

cite_count <- 15
tab$cite <- seq(cite_count,cite_count+nrow(tab)-1)
tab$Example <- paste0(tab$Example," [",tab$cite,"]")

ft <- flextable(tab[,1:4])
ft <- bg(ft, bg = "#A6A6A6", part = "header")
ft <- bold(ft, part = "header")
ft <- bold(ft, j = 1, part = "body")
ft <- align(ft, align = "center", part = "all" )
ft <- bg(ft, i = ~ seq(from = 1, to = nrow(tab)) %% 2 == 0, bg = "#DDDDDD", part = "body")
ft <- fontsize(ft, size = 7, part = "all")
ft <- set_table_properties(ft, layout = "autofit")
ft <- align(ft, j = 4, align = "left", part = "body")

ft

```

#####

# Methods

## Protocol and ethics 

A protocol for this analysis was registered in advance and followed at all stages of the study.[@mcguinness2020] Any deviations from the protocol are described. Ethical approval was not required for this study. 

## Data extraction

The data availability statements of preprints posted on the medRxiv preprint repository between 25th June 2019 (the date of first publication of a preprint on medRxiv) and 1st May 2020 were extracted using the `medrxivr` and `rvest` R packages.[@medrxivr; @rvest] Details on the journal in which preprints were subsequently published was extracted using the published DOI provided by medRxiv and  `rcrossref`.[@rcrossref] Several other R packages were used for data cleaning and analysis. [@base; @devtools; @dplyr; @flextable; @ggplot2; @grateful; @here; @irr; @officer; @patchwork; @RColorBrewer; @rio; @stringr; @tibble]

The data availability statements for published articles were extracted manually into an Excel file, and are available for inspection (see Material availability section).

## Analysis

A classification system was developed to categorize each data availability statement as either open or closed, with additional ordered sub-categories indicating the degree of openness (see Table 1). The system was based on the Findability and Accessibility elements of the FAIR framework,[@wilkinson2016] the categories used by previous effort to categorize published data availability statements,[@colavizza2020; @federer2018] and discussion with colleagues. The data availability statement for each preprinted record were categorized by two independent researchers, using the groups presented in Table \@ref(tab:categorylabels), while the statements for published articles were categorized using all groups barring Category 3 and 4 ("Available in the future").  Researchers were provided only with the data availability statement, and as a result, were blind to the associated preprint metadata (e.g. title, authors, corresponding author institution) in case this could affect their assessments. Any disagreements were resolved through discussion. Due to our large sample, if authors claimed that all data were available in the manuscript or as a supplemental file, or that their study did not make use of any data, we took them at their word. Where a data availability statement met multiple categories, or contained multiple data sources with varying levels of openness, we took a conservative approach and categorized it on the basis of the most restrictive aspect (see Supplementary Materials 3 for some illustrative examples). We plotted the distribution of preprint and published data availability statements across the nine categories presented in Table \@ref(tab:categorylabels). Records for which the data availability statement was categorized as "Not applicable" (Category 1 from Table \@ref(tab:categorylabels)) at either the preprint or published stage were excluded from further analyses.

To assess if data availability statements change between preprint and published articles, we examined whether a discrepancy existed between the categories assigned to the preprinted and published statements, and the direction of the discrepancy (more "closed" or more "open"). We declare a minor deviation from our protocol,[@mcguinness2020] in relation to this analysis. Rather than investigating the data-sharing policy only for journals with the greatest change in openness, we extracted and categorized the data-sharing policies for all journals for which preprints had subsequently been published using two categories (1: "requiring/mandating data sharing" and, 2: "not requiring/mandating data sharing"), and compared the change in openness between these two categories. 

To assess claims that data will be provided on publication, the data availability statements accompanying the published articles for all records in Category 3 ("Data available on publication (link provided)") or Category 4 ("Data available on publication (no link provided)") from Table \@ref(tab:categorylabels) were assessed, and any difference between the two categories examined. Finally, to assess whether data availability statements also capture code availability, the data availability statement and full text PDF for a random sample 400 preprinted records were assessed for code availability (1: "code availability described" and 2: "code availability not described"). 

## Patient and public involvement
Due to the study design and topic, patients and the public were not involved in the choice of research question, the design of the study, the conduct of the study, the interpretation of the results, or our dissemination plans. Dissemination to participants is not applicable.

#####

# Results {#results}

The data availability statements accompanying `r n_records` preprints registered  between 25th June 2019 and 1st May 2020 were extracted from the medRxiv preprint repository on the 26th May 2020 and were coded according to the categories in Table \@ref(tab:categorylabels). During this process, agreement between raters was high (Cohen's Kappa = `r kappa`; "almost perfect agreement"). 

Of the `r n_records` preprints, `r n_excluded_pre` in Category 0 ("Not applicable") were excluded following coding, leaving `r n_total_pre` remaining records. Of these, `r n_available_pre` had made their data open as per the criteria in Table \@ref(tab:categorylabels). The distribution of data availability statements across the categories can be seen in Figure \@ref(fig:fig-distrib). A total of `r n_pub` preprints had been subsequently published, and of these, only `r n_pub_denominator_format` had data availability statements that we could categorize. `r n_excluded_pub` records in Category 0 ("Not applicable") were excluded, and of the `r n_total_pub` remaining, `r n_available_pub` had made their data open as per our criteria.

&nbsp;

```{r fig-distrib, fig.width = 10, fig.cap="_Distribution of the data availability statements of preprinted (Panel I) and published (Panel II) records by category from Table 1._"}
plot_s1
```

&nbsp;

For the comparison of preprint data availability statements with their published counterparts, we excluded records that were not published, that did not have a published data availability statement or that were labeled as "Not applicable" at either the preprint or published stage, leaving `r n_comparison` records. When grouped by data-sharing policy, there was a greater change towards open data availability statements in journals requiring/mandating data sharing versus those that encouraged it (Table \@ref(tab:tabs1)). Moreover, the data availability statements for `r n_more_closed` articles published in journals that did not require open data sharing, became less open on publication ((Table \@ref(tab:tabs1)). The change in openness for preprints grouped by category, and stratified by journal policy, is shown in Supplementary Table 1, while the change  for each individual journal is shown in Supplementary Table 2.

&nbsp;

```{r tabs1, ft.aign = "left", tab.cap = "Change in openness of data availability statements from preprint to published article, grouped by journal data-sharing policy."}

ft_s1

```

&nbsp;

`r n_future` preprints stated that data would be available on publication, but only `r n_future_pub` of these had subsequently been published (Table \@ref(tab:tabs3)) and openness on publication did not seem to vary based on whether the preprinted data availability statements include a link to an embargoed repository or not (though the sample size is small).

&nbsp;

```{r tabs3, ft.align="center", tab.cap='Assessment of whether researchers promising to make data available on publication actually do so, and whether this differs if researchers included a link to an embargoed repository or not.'}

ft_s3

```

&nbsp;

Of the 400 records for which code availability was assessed, `r code_pdf_total` mentioned code availability in their full text manuscripts. Of these, only `r code_captured` also reported this in their data availability statements (Table \@ref(tab:tabs2)).

&nbsp;

```{r tabs2, ft.align="center", tab.cap='Comparison of code availability declarations between data availability statements and full text manuscripts.'}

ft_s2

```

&nbsp;


#####

# Discussion

## Principal findings and comparison with other studies

We have reviewed `r n_records` preprinted and `r n_pub_denominator` published data availability statements, coding them as "open" or "closed" according to a predefined classification system. During this labor-intensive process, we appreciated statements that reflected the authors' enthusiasm for data sharing ("YES"),[@chen2020] their bluntness ("Data is not available on request."),[@hashmi2020] and their efforts to endear themselves to the reader ("I promise all data referred to in the manuscript are available.").[@peng2020] Of the preprinted statements, almost three-quarters were categorized as "closed", with the largest individual category being "available on request" In light of the substantial impact that studies published as preprints on medRxiv have had on real-time decision making during the current COVID-19 pandemic,[@fraser2020] it is concerning that data for these preprints is so infrequently readily available for inspection.

A minority of published records we examined contained a data availability statement (n = `r n_pub_denominator_format`). This lack of availability statement at publication results in a loss of useful information. For at least one published article, we identified relevant information in the preprinted statement that did not appear anywhere in the published article, due to it not containing a data availability statement.[@martin2019; @martin2020] 

We provide initial descriptive evidence that strict data-sharing policies, which require data to be made openly available (where appropriate) as a condition of publication, appear to succeed in making research data more open than those that simply encourage data sharing. Our findings, though based on a relatively small number of observations, agree with other studies on the effect of journal policies on author behavior. Recent work has shown that "requiring" a data availability statement was effective in ensuring that this element was completed,[@federer2018] while "encouraging" authors to follow a reporting checklist (the ARRIVE checklist) had no effect on compliance.[@hair2019; @kilkenny2010] 

Finally, we also provide evidence that data availability statements alone are insufficient to capture code availability declarations. Code sharing has been advocated strongly elsewhere,[@goldacre2019; @eglen2017] as it provides an insight into the analytic decisions made by the research team, and there are few (if any) circumstances in which it is not possible to share the analytic code underpinning an analysis. Similar to data availability statements, a dedicated code availability statement which is critically assessed as part of the publication process will help researchers to appraise published results. 

## Strengths and limitations

A particular strength of this analysis is that the design allows us to compare what is essentially the same paper (same design, findings and authorship team) under two different data-sharing polices, and assess the change in the statement between them. To our knowledge this is the first study to use this approach to examine the impact of journal editorial policies. This approach also allows us to address the issue of self-selection. When looking at published articles alone, it is not possible to tell whether authors always intended to make their data available and chose the journal for because of its reputation for open data sharing. In addition, we have examined all available preprints within our study period and all corresponding published articles, rather than taking a sub-sample. Finally, categorization of the statements was carried out by two independent researchers using predefined categories, reducing the risk of misclassification.

However, our analysis is subject to a number of potential limitations. The primary one is that manuscripts (at both the preprint and published stages) may have included links to the data, or more information that uniquely identifies the dataset from a data portal, within the text (for example, in the Methods section). While this might be the case, if readers are expected to piece together the relevant information from different locations in the manuscript, it throws into question what having a dedicated data availability statement adds. A second limitation is that we do not assess the veracity of any data availability statements, which may introduce some misclassification bias into our categorization. For example, we do not check whether all relevant data can actually be found in the manuscript/supplementary files (Category 7) or the linked repository (Category 8). Previous work has suggested that this is unlikely to be the case.[@roche2015] A final limitation is that for Categories 1 ("No data available"() and 2 ("Available on request"), there will be situations where making research data available is not feasible, for example, due to cost or concerns about patient re-identifiability.[@goodhill2014; @courbier2019] This is situation is perfectly reasonable, as long as statements are explicit in justifying the lack of open data.

## Implications for policy

Based on our analysis, journals that require data sharing succeed in making research data more open between preprint and publication. This would suggest that data sharing could be immediately improved by journals becoming more stringent in their data availability policies. Similarly, introduction of a related code availability section (or composite "material" availability section) will aid in reproducibility by capturing whether analytic code is available in a standardized manuscript section.

## Conclusion

Data availability statements are an important tool in the fight to make studies more reproducible. However, without critical review of these statements in line with strict data-sharing policies, authors default to making data "available on request". As such, we would strongly encourage all journals to take a moment this holiday season and reassess their data-sharing policies. 

However, while we have focused on primarily on the role of journals above, some responsibility for enacting change rests with the research community at large. If we researchers regularly shared our data, strict journal data sharing policies would not be needed. As such, we would encourage authors to make a resolution to share the data accompanying an upcoming publication, regardless of whether the journal actually requires it. In the words of Bob Geldof's researcher alter-ego: “(heal the world), let us see your datasets next time”. 
 
#####

# Highlights

## What is already known on this topic

- Data sharing is increasingly seen as a core component of good research practice.

- Data availability statements are completed by researchers when required, but by themselves, do not encourage researchers to make their data publicly available.

## What this study adds

- Similar to published articles, preprinted data availability statements most commonly claim to make data "available on request".

- Strict editorial policies that mandate data sharing (where appropriate) as a condition of publication appear to be effective in making research data available.

#####

# Back Matter

## Material available statement

All materials (data, code and supporting information) are available on request to one S. Claus (or alternatively can be found at https://github.com/mcguinlu/data-availability-impact, archived at time of submission on Zenodo (DOI: 10.5281/zenodo.3968301)). 

## Contributorship statement

__CReditT Taxonomy__  
Conceptualization: Luke A. McGuinness.
Data Curation: Luke A. McGuinness.
Formal Analysis: Luke A. McGuinness and Athena L. Sheppard.  
Investigation: Luke A. McGuinness and Athena L. Sheppard.  
Methodology: Luke A. McGuinness and Athena L. Sheppard.  
Project Administration: Luke A. McGuinness.  
Software: Luke A. McGuinness.  
Supervision: Luke A. McGuinness.  
Validation: Luke A. McGuinness and Athena L. Sheppard.  
Visualization: Luke A. McGuinness.  
Writing - Original Draft Preparation: Luke A. McGuinness.  
Writing - Review & Editing: Luke A. McGuinness and Athena L. Sheppard.  

## Transparency statement

All authors reviewed this manuscript before approving the final version. LAM is guarantor of the article, affirms that this manuscript is an honest, accurate, and transparent account of the study being reported; that no important aspects of the study have been omitted; and that any discrepancies from the study as planned (and, if relevant, registered) have been explained.

## Acknowledgements

We must acknowledge the input of several people, without whom the quality of this work would have been diminished: Matthew Grainger, Alfredo Sánchez-Tójar and Neal Haddaway for their insightful comments on the subject of data availability statements; Antica Culina, Phil Gooch and Sarah Nevitt for their skill in identifying missing published papers based on the vaguest of descriptions; and Ciara Gardiner, for proof-reading this manuscript. 


## Role of funders

We have read and understood BMJ policy on declaration of interests and declare LAM is supported by an National Institute for Health Research (NIHR) Doctoral Research Fellowship (DRF-2018-11-ST2-048). The funder had no role in designing the study; in the collection, analysis, and interpretation of data; in the writing of the report; and in the decision to submit the article for publication. The views expressed in this article are those of the authors and do not necessarily represent those of the NHS, the NIHR, MRC, or the Department of Health and Social Care.

## Competing interest statement

All authors have completed the ICMJE uniform disclosure form and declare: no support from any organisation for the submitted work; no financial relationships with any organisations that might have an interest in the submitted work in the previous three years, no other relationships or activities that could appear to have influenced the submitted work.

## Licence

The Corresponding Author has the right to grant on behalf of all authors and does grant on behalf of all authors, a worldwide licence (http://www.bmj.com/sites/default/files/BMJ%20Author%20Licence%20March%202013.doc) to the Publishers and its licensees in perpetuity, in all forms, formats and media (whether known now or created in the future), to i) publish, reproduce, distribute, display and store the Contribution, ii) translate the Contribution into other languages, create adaptations, reprints, include within collections and create summaries, extracts and/or, abstracts of the Contribution and convert or allow conversion into any format including without limitation audio, iii) create any other derivative work(s) based in whole or part on the on the Contribution, iv) to exploit all subsidiary rights to exploit all subsidiary rights that currently exist or as may exist in the future in the Contribution, v) the inclusion of electronic links from the Contribution to third party material where-ever it may be located; and, vi) licence any third party to do any or all of the above. 

#####

# References


